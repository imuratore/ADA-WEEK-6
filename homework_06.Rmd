---
title: "homework 04"
author: "Isabella Muratore"
output:
  html_document: default
  html_notebook: default
---
# homework 04

## [1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.

### This function has 6 possible outcomes in terms of the settings used for calculations depending on whether this is a two sample or one sample test (based on how many arguments are handed) and whether the test is two-sided, greater than or less than. It returns a list of Z, P, and confidence intervals.
```{r}
# p1/p2 are vectors of values for one or more samples. n1/n2 are the sizes of the samples. p0 is used for comparison in a one-sided test. 
Z.prop.test <- function(p1,p2=NULL,n1,n2=NULL,p0,alternative="two.sided",correct=TRUE,conf.level=0.95){
  if (alternative == "two.sided"){
    if (is.null(p2) | is.null(n2)){
      #defining mean of the values in p1
      phat <- mean(p1)
      
      if ((n1*phat1<5) | (n1*(1−phat1)<5)){
        #testing sample size
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      #two-sided test with one sample
      phat <- mean(p1)
      
      #z statistic
      z <- (phat - p0)/sqrt(p0 * (1 - p0)/n1)
      
      #p value
      p <- 1- pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
      
      #confidence interval
      lower <- phat - qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
      upper <- phat + qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
      ci <- c(lower, upper)

    } else {

      phat1 <- mean(p1)

      phat2 <- mean(p2)
      
      if ((n1*phat1<5) || (n1*(1−phat1)<5) || (n2*phat2<5) || (n2*(1−phat2)<5)){
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      #two-sided test with two samples
      
      pstar <- (sum(p1) + sum(p2))/(n1 + n2)

      phat1 <- mean(p1)

      phat2 <- mean(p2)

      z <- (phat2 - phat1)/sqrt(pstar * (1 - pstar) * (1/n1 + 1/n2))

      p <- 1- pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)

      lower <- (phat2-phat1) - qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
      upper <- (phat2-phat1) + qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
      ci <- c(lower, upper)

    } 
    
  } else {
    if (is.null(p2) | is.null(n2)){
      phat <- mean(p1)
      if ((n1*phat1<5) | (n1*(1−phat1)<5)){
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      if (alternative == "greater"){
        #one-sided greater than test with one sample
        phat <- mean(p1)
        
        z <- (phat - p0)/sqrt(p0 * (1 - p0)/n1)

        p <- pnorm(z, lower.tail = FALSE)

        lower <- phat - qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        upper <- phat + qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        ci <- c(lower, upper)

      } else {
        #one-sided less than test with one sample
        phat <- mean(p1)
        
        z <- (phat - p0)/sqrt(p0 * (1 - p0)/n1)

        p <- pnorm(z, lower.tail = TRUE)

        lower <- phat - qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        upper <- phat + qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        ci <- c(lower, upper)

      }
      
    } else {
      phat1 <- mean(p1)

      phat2 <- mean(p2)
      
      if ((n1*phat1<5) || (n1*(1−phat1)<5) || (n2*phat2<5) || (n2*(1−phat2)<5)){
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      #one-sided greater than test with two samples
      if (alternative == "greater"){
        pstar <- (sum(p1) + sum(p2))/(n1 + n2)

        phat1 <- mean(p1)

        phat2 <- mean(p2)

        z <- (phat2 - phat1)/sqrt(pstar * (1 - pstar) * (1/n1 + 1/n2))

        p <- pnorm(z, lower.tail = FALSE)

        lower <- (phat2-phat1) - qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        upper <- (phat2-phat1) + qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        ci <- c(lower, upper)
      } else{
        #one-sided less than test with two samples
        pstar <- (sum(p1) + sum(p2))/(n1 + n2)

        phat1 <- mean(p1)

        phat2 <- mean(p2)

        z <- (phat2 - phat1)/sqrt(pstar * (1 - pstar) * (1/n1 + 1/n2))

        p <- pnorm(z, lower.tail = TRUE)
        
        lower <- (phat2-phat1) - qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        upper <- (phat2-phat1) + qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        ci <- c(lower, upper)
      }

    }
    
  }
  #returning list of parameters preceded by their names
  output <- list("z", z, "p", p, "ci", ci)
  output
  
}

```
## The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size)


```{r}
#importing data set
library(curl)
library(ggplot2)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/KamilarAndCooperData.csv")
#making data frame
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
#creating model I linear model for longevity by brain size
m <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
m
#plotting the linear model, adding scatterpoints, adding linear model line and 90% prediction intervals, adding text describing linear model using slope and intercept generated by lm()
g <- ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x, level=0.90)
g <- g + annotate("text", x=300, y=350, label= "y = 1.218*x + 248.952")
g
```

```{r}
m

# β1 (slope coefficient) = 0.2341
```
###
```{r}

#finding conf. intervals at 90% and 10% for the slope parameter (multiple parameters are outputted)

mI <- confint.lm(object=m, level=0.80)

mI

#taking slope stats

ci <- c(1.076163, 1.359817)

ci
```

```{r}

m <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)

#making confidence interval and prediction for y value based on x value, here 800 for x value, based on the linear model for the longevity by brain size

ci_800 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "confidence", 
    level = 0.90) 

ci_800


```
### prediction = 1223.345 months, ci = 1089.461-1357.228

```{r}
#creating model I linear model for log of longevity by brain size
m <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)
#plotting the linear model, adding scatterpoints, adding linear model line and 90% prediction intervals, adding text describing linear model using slope and intercept generated by lm()
g <- ggplot(data = d, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m)))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x, level=0.90)
g <- g + annotate("text", x=3.75, y=5, label= "y = 0.2341*x + 4.8790")
g 
```
### It appear that the log model fits the data better because rather than showing a clump of points with a few outliers, here the points are relatively evenly distributed over the plot, such that the prediction interval can remain slim across its whole length because there are enough points at each location to make these predictions accurate.
```{r}
m

# β1 (slope coefficient) = 0.2341
```
### Finding the confidence interval for the slope parameter under the log model
```{r}
mI <- confint.lm(object=m, level=0.80)

mI

#taking slope stats

ci <- c(0.2112063, 0.2570929)

ci
```
### Using the log model find prediction and confidence interval for  longevity for a species with average brain mass of 800g.
```{r}
m <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)

#making point prediction for y value based on x value, here 800 for x value, based on the linear model for the log of longevity by brain size

ci_800 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "confidence", 
    level = 0.90) 

ci_800

#adjusting returned values to get number of months from log number of months

prediction <- exp(6.44415)
prediction
lower <- exp(6.348235)
upper <- exp(6.540065)
ci <- c(lower,upper)
ci

```
### The predicted values are for the log of longevity in months, so the predictions are returned as the log of the actual number of months. Taking the inverse log of these values yields 629.0118 as the prediction of longevity, between 571.4832 and 692.3316. This is lower than the prediction using the non-log model.

## Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

### I do not very much trust these models to predict longevity for an 800g brain because 800 is well past the last x value included in this data set, so the relationship could change after this point and for example, flatten off since animals are not going to live forever even with large brains. 

## Looking at your two models, which do you think is better? Why?

### Copied from above:  It appear that the log model fits the data better because rather than showing a clump of points with a few outliers, here the points are relatively evenly distributed over the plot, such that the prediction interval can remain slim across its whole length because there are enough points at each location to make these predictions accurate.

### ALSO, taking the correlation between the complete pairwise data for these variables shows a stronger correlation for the log model, so this model has better predictive ability:

```{r}
cor(d$Brain_Size_Female_Mean,d$MaxLongevity_m, use="pairwise.complete.obs")

cor(log(d$Brain_Size_Female_Mean),log(d$MaxLongevity_m), use="pairwise.complete.obs")
```


