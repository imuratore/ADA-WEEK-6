---
title: "homework 04"
author: "Isabella Muratore"
output:
  html_document: default
  html_notebook: default
---
# homework 04

## [1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.

### This function has 6 possible outcomes in terms of the settings used for calculations depending on whether this is a two sample or one sample test (based on how many arguments are handed) and whether the test is two-sided, greater than or less than. It returns a list of Z, P, and confidence intervals.
```{r}
# p1/p2 are vectors of values for one or more samples. n1/n2 are the sizes of the samples. p0 is used for comparison in a one-sided test. 
Z.prop.test <- function(p1,p2=NULL,n1,n2=NULL,p0,alternative="two.sided",correct=TRUE,conf.level=0.95){
  if (alternative == "two.sided"){
    if (is.null(p2) | is.null(n2)){
      #defining mean of the values in p1
      phat <- mean(p1)
      
      if ((n1*phat1<5) | (n1*(1−phat1)<5)){
        #testing sample size
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      #two-sided test with one sample
      phat <- mean(p1)
      
      #z statistic
      z <- (phat - p0)/sqrt(p0 * (1 - p0)/n1)
      
      #p value
      p <- 1- pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
      
      #confidence interval
      lower <- phat - qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
      upper <- phat + qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
      ci <- c(lower, upper)

    } else {

      phat1 <- mean(p1)

      phat2 <- mean(p2)
      
      if ((n1*phat1<5) || (n1*(1−phat1)<5) || (n2*phat2<5) || (n2*(1−phat2)<5)){
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      #two-sided test with two samples
      
      pstar <- (sum(p1) + sum(p2))/(n1 + n2)

      phat1 <- mean(p1)

      phat2 <- mean(p2)

      z <- (phat2 - phat1)/sqrt(pstar * (1 - pstar) * (1/n1 + 1/n2))

      p <- 1- pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)

      lower <- (phat2-phat1) - qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
      upper <- (phat2-phat1) + qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
      ci <- c(lower, upper)

    } 
    
  } else {
    if (is.null(p2) | is.null(n2)){
      phat <- mean(p1)
      if ((n1*phat1<5) | (n1*(1−phat1)<5)){
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      if (alternative == "greater"){
        #one-sided greater than test with one sample
        phat <- mean(p1)
        
        z <- (phat - p0)/sqrt(p0 * (1 - p0)/n1)

        p <- pnorm(z, lower.tail = FALSE)

        lower <- phat - qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        upper <- phat + qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        ci <- c(lower, upper)

      } else {
        #one-sided less than test with one sample
        phat <- mean(p1)
        
        z <- (phat - p0)/sqrt(p0 * (1 - p0)/n1)

        p <- pnorm(z, lower.tail = TRUE)

        lower <- phat - qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        upper <- phat + qnorm(conf.level) * sqrt(phat * (1 - phat)/n1)
        ci <- c(lower, upper)

      }
      
    } else {
      phat1 <- mean(p1)

      phat2 <- mean(p2)
      
      if ((n1*phat1<5) || (n1*(1−phat1)<5) || (n2*phat2<5) || (n2*(1−phat2)<5)){
        message("Warning, n*p and n*p-1 should be greater than 5")
      }
      #one-sided greater than test with two samples
      if (alternative == "greater"){
        pstar <- (sum(p1) + sum(p2))/(n1 + n2)

        phat1 <- mean(p1)

        phat2 <- mean(p2)

        z <- (phat2 - phat1)/sqrt(pstar * (1 - pstar) * (1/n1 + 1/n2))

        p <- pnorm(z, lower.tail = FALSE)

        lower <- (phat2-phat1) - qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        upper <- (phat2-phat1) + qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        ci <- c(lower, upper)
      } else{
        #one-sided less than test with two samples
        pstar <- (sum(p1) + sum(p2))/(n1 + n2)

        phat1 <- mean(p1)

        phat2 <- mean(p2)

        z <- (phat2 - phat1)/sqrt(pstar * (1 - pstar) * (1/n1 + 1/n2))

        p <- pnorm(z, lower.tail = TRUE)
        
        lower <- (phat2-phat1) - qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        upper <- (phat2-phat1) + qnorm(conf.level) * sqrt(phat1*(1-phat1)/n1+phat2*(1-phat2)/n2)
        ci <- c(lower, upper)
      }

    }
    
  }
  #returning list of parameters preceded by their names
  output <- list("z", z, "p", p, "ci", ci)
  output
  
}

```
## The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size)


```{r}
#importing data set
library(curl)
library(ggplot2)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/KamilarAndCooperData.csv")
#making data frame
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
#creating model I linear model for longevity by brain size
m <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
m

#intercept = 248.952
#slope = 1.218

#Using the information about the linear model plus the prediction for y at x=0 to create linear models for the 90% prediction intervals, since ggplot2 does not have an automatic prediction interval function.

#y = (1.218*x+248.952)

ci_0 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 0), interval = "predict", 
    level = 0.90) 

ci_0

#intercept 90% PIs: 97.18861 400.7159

# 248.952-97.18861=151.7634
#This is the value by which I will offset the upper and lower 90% PI lines

#plotting the linear model, adding scatterpoints, adding linear model line, 90% prediction intervals, and 90% confidence intervals, adding text describing linear model using slope and intercept generated by lm()


g <- ggplot(d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m))
g <- g + geom_point()
g <- g + geom_line(aes(x = Brain_Size_Species_Mean, y = ((1.218*Brain_Size_Species_Mean+248.952)-151.7634)), color = "red", linetype = "dashed")
g <- g + geom_line(aes(x = Brain_Size_Species_Mean, y = ((1.218*Brain_Size_Species_Mean+248.952)+151.7634)), color = "red", linetype = "dashed")
g <- g + geom_smooth(method = "lm", formula = y ~ x, level=0.90)
g <- g + annotate("text", x=300, y=350, label= "y = 1.218*x + 248.952 red=predict, blue/gray=confidence")
g


```
## Identify and interpret the point estimate of the slope (β1β1), as well as the outcome of the test associated with the hypotheses H0: β1β1 = 0; HA: β1β1 ≠ 0. Also, find a 90 percent CI for the slope (β1β1) parameter.
```{r}

library(lmodel2)

mI <- lmodel2(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d, range.y = "relative", range.x = "relative", nperm = 1000)

mI

#For slope point estimate see above. 
#90% slope CI = 1.000130, 1.435851

summary(m)

#P-value for slope!=0: <2e-16
#Therefore we reject the null that slope=0

```

### Using x value to predict y under the non-log model. 
```{r}

m <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)

#making confidence interval and prediction for y value based on x value, here 800 for x value, based on the linear model for the longevity by brain size

ci_800 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "predict", 
    level = 0.90) 

ci_800


```
### prediction = 1223.345 months, ci = 1021.805-1424.884

```{r}
#creating model I linear model for log of longevity by brain size
m <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)
m

#intercept = 4.8790
#slope = 0.2341

#y = (0.2341*x+4.8790)

ci_800 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 0), interval = "predict", 
    level = 0.90) 

ci_800

#intercept 90% PIs: 6.021424 6.866876
#6.44415-6.021424=0.422726
#This is the value by which I will offset the upper and lower 90% PI lines

#plotting the log linear model, adding scatterpoints, adding linear model line, 90% prediction intervals, and 90% confidence intervals, adding text describing linear model using slope and intercept generated by lm()
g <- ggplot(d, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m)))
g <- g + geom_point()
g <- g + geom_line(aes(x = log(Brain_Size_Species_Mean), y = ((0.2341*log(Brain_Size_Species_Mean)+4.8790)-0.422726)), color = "red", linetype = "dashed")
g <- g + geom_line(aes(x = log(Brain_Size_Species_Mean), y = ((0.2341*log(Brain_Size_Species_Mean)+4.8790)+0.422726)), color = "red", linetype = "dashed")
g <- g + geom_smooth(method = "lm", formula = y ~ x, level=0.90)
g <- g + annotate("text", x=3, y=5, label= "y = 0.2341*x+4.8790 red=predict, blue/gray=confidence")
g

```


### It appear that the log model fits the data better because rather than showing a clump of points with a few outliers, here the points are relatively evenly distributed over the plot, such that the prediction interval can remain slim across its whole length because there are enough points at each location to make these predictions accurate.

## ## Identify and interpret the point estimate of the slope (β1β1), as well as the outcome of the test associated with the hypotheses H0: β1β1 = 0; HA: β1β1 ≠ 0. Also, find a 90 percent CI for the slope (β1β1) parameter.
```{r}
mI <- lmodel2(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d, range.y = "relative", range.x = "relative", nperm = 1000)

mI

#For slope point estimate see above. 
#90% slope CI = 0.1989063, 0.2693928

summary(m)

#P-value for slope!=0: <2e-16
#Therefore we reject the null that slope=0

```
### Using the log model find prediction and confidence interval for  longevity for a species with average brain mass of 800g.
```{r}
m <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)

#making point prediction for y value based on x value, here 800 for x value, based on the linear model for the log of longevity by brain size

ci_800 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "predict", level = 0.90) 

ci_800

#adjusting returned values to get number of months from log number of months

prediction <- exp(6.44415)
prediction
lower <- exp(66.021424)
upper <- exp(6.866876)
ci <- c(lower,upper)
ci

```
### The predicted values are for the log of longevity in months, so the predictions are returned as the log of the actual number of months. Taking the inverse log of these values yields 629.0118 as the prediction of longevity, between 571.4832 and 692.3316. This is lower than the prediction using the non-log model.

## Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

### I do not very much trust these models to predict longevity for an 800g brain because 800 is well past the last x value included in this data set, so the relationship could change after this point and for example, flatten off since animals are not going to live forever even with large brains. 

## Looking at your two models, which do you think is better? Why?

### Copied from above:  It appear that the log model fits the data better because rather than showing a clump of points with a few outliers, here the points are relatively evenly distributed over the plot, such that the prediction interval can remain slim across its whole length because there are enough points at each location to make these predictions accurate.

### ALSO, taking the correlation between the complete pairwise data for these variables shows a stronger correlation for the log model, so this model has better predictive ability:

```{r}
cor(d$Brain_Size_Female_Mean,d$MaxLongevity_m, use="pairwise.complete.obs")

cor(log(d$Brain_Size_Female_Mean),log(d$MaxLongevity_m), use="pairwise.complete.obs")
```


